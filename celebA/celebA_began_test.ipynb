{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import misc\n",
    "import os\n",
    "# import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, s):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, s, s, 1], padding = 'SAME')\n",
    "def deconv2d(x, W, output_shape, s):\n",
    "    return tf.nn.conv2d_transpose(x, W, output_shape, strides = [1, s, s, 1], padding = 'SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(img_path):\n",
    "#     img = misc.imread(img_path)/255. - 0.5\n",
    "    img = misc.imread(img_path)\n",
    "#     img = img[..., ::-1]  # rgb to bgr\n",
    "    return img\n",
    "\n",
    "def batch(batch_size, ls):\n",
    "    index = (np.random.sample(batch_size)*202599).astype(int)\n",
    "    ls_x = ls[[index]]\n",
    "    return np.array([np.reshape(misc.imread(\"./img_align_celeba/\"+x), 116412)/255. for x in ls_x])\n",
    "\n",
    "class layer:\n",
    "    def __init__(self, in_size, out_size):\n",
    "        self.W = tf.Variable(tf.random_normal([in_size, out_size], mean=0.0, stddev=0.1))\n",
    "        self.b = tf.Variable(tf.random_normal([1, out_size], mean=0.0, stddev=0.1))\n",
    "    def output(self, inputs, activation_function=None):\n",
    "        if activation_function == None:\n",
    "            return tf.matmul(inputs, self.W) + self.b\n",
    "        else :\n",
    "            return activation_function(tf.matmul(inputs, self.W) + self.b)\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.random_normal(shape, mean=0.0, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.random_normal(shape, mean=0.0, stddev=0.1)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "g_dim = 128\n",
    "\n",
    "ls = np.array(os.listdir(\"./img_align_celeba\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_e1 = layer(28*23*72, 1024)\n",
    "layer_e2 = layer(1024, g_dim)\n",
    "\n",
    "layer_d1 = layer(g_dim, 1024)\n",
    "layer_d2 = layer(1024, 28*23*72)\n",
    "\n",
    "layer_g1 = layer(g_dim, 1024)\n",
    "layer_g2 = layer(1024, 28*23*72)\n",
    "\n",
    "encoder_var = {\n",
    "    \"W_e_conv1\" : weight_variable([3,3, 3,24]),\n",
    "    \"W_e_conv2\" : weight_variable([3,3,24,24]),\n",
    "    \"W_e_conv3\" : weight_variable([3,3,24,48]),\n",
    "    \"W_e_conv4\" : weight_variable([3,3,48,48]),\n",
    "    \"W_e_conv5\" : weight_variable([3,3,48,72]),\n",
    "    \"W_e_conv6\" : weight_variable([3,3,72,72]),\n",
    "#     \"W_e_conv3\" : weight_variable([3,3,64,128]),\n",
    "    \"b_e_conv1\" : bias_variable([24]),   \n",
    "    \"b_e_conv2\" : bias_variable([24]),\n",
    "    \"b_e_conv3\" : bias_variable([48]),\n",
    "    \"b_e_conv4\" : bias_variable([48]),   \n",
    "    \"b_e_conv5\" : bias_variable([72]),\n",
    "    \"b_e_conv6\" : bias_variable([72])\n",
    "#     \"b_e_conv3\" : bias_variable([128])\n",
    "}\n",
    "\n",
    "decoder_var = {\n",
    "    \"W_d_conv1\" : weight_variable([3,3,72,72]),\n",
    "    \"W_d_conv2\" : weight_variable([3,3,48,72]),\n",
    "    \"W_d_conv3\" : weight_variable([3,3,48,48]),\n",
    "    \"W_d_conv4\" : weight_variable([3,3,24,48]),\n",
    "    \"W_d_conv5\" : weight_variable([3,3,24,24]),\n",
    "    \"W_d_conv6\" : weight_variable([3,3, 3,24]),\n",
    "    \n",
    "    \"b_d_conv1\" : bias_variable([72]),\n",
    "    \"b_d_conv2\" : bias_variable([48]),\n",
    "    \"b_d_conv3\" : bias_variable([48]),\n",
    "    \"b_d_conv4\" : bias_variable([24]),\n",
    "    \"b_d_conv5\" : bias_variable([24]),\n",
    "    \"b_d_conv6\" : bias_variable([3])\n",
    "}\n",
    "\n",
    "generator_var = {\n",
    "    \"W_g_conv1\" : weight_variable([3,3,72,72]),\n",
    "    \"W_g_conv2\" : weight_variable([3,3,48,72]),\n",
    "    \"W_g_conv3\" : weight_variable([3,3,48,48]),\n",
    "    \"W_g_conv4\" : weight_variable([3,3,24,48]),\n",
    "    \"W_g_conv5\" : weight_variable([3,3,24,24]),\n",
    "    \"W_g_conv6\" : weight_variable([3,3, 3,24]),\n",
    "    \n",
    "    \"b_g_conv1\" : bias_variable([72]),\n",
    "    \"b_g_conv2\" : bias_variable([48]),\n",
    "    \"b_g_conv3\" : bias_variable([48]),\n",
    "    \"b_g_conv4\" : bias_variable([24]),\n",
    "    \"b_g_conv5\" : bias_variable([24]),\n",
    "    \"b_g_conv6\" : bias_variable([3])\n",
    "}\n",
    "\n",
    "var_d = [encoder_var[e] for e in encoder_var]+[decoder_var[d] for d in decoder_var]+[layer_e1.W, layer_e1.b, layer_e2.W, layer_e2.b, layer_d1.W, layer_d1.b, layer_d2.W, layer_d2.b]\n",
    "var_g = [generator_var[g] for g in generator_var]+[layer_g1.W, layer_g1.b, layer_g2.W, layer_g2.b]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    x_origin = tf.reshape(x, [-1,218,178,3])      #28x28x1\n",
    "    h_e_conv1 = tf.nn.relu(tf.add(conv2d(x_origin, encoder_var[\"W_e_conv1\"], 1), encoder_var[\"b_e_conv1\"]))     #14x14x16\n",
    "    h_e_conv2 = tf.nn.relu(tf.add(conv2d(h_e_conv1, encoder_var[\"W_e_conv2\"], 2), encoder_var[\"b_e_conv2\"]))    #7x7x32\n",
    "    \n",
    "    h_e_conv3 = tf.nn.relu(tf.add(conv2d(h_e_conv2, encoder_var[\"W_e_conv3\"], 1), encoder_var[\"b_e_conv3\"]))    #4x4x64\n",
    "    h_e_conv4 = tf.nn.relu(tf.add(conv2d(h_e_conv3, encoder_var[\"W_e_conv4\"], 2), encoder_var[\"b_e_conv4\"]))    #4x4x64\n",
    "    \n",
    "    h_e_conv5 = tf.nn.relu(tf.add(conv2d(h_e_conv4, encoder_var[\"W_e_conv5\"], 1), encoder_var[\"b_e_conv5\"]))    #4x4x64\n",
    "    h_e_conv6 = tf.nn.relu(tf.add(conv2d(h_e_conv5, encoder_var[\"W_e_conv6\"], 2), encoder_var[\"b_e_conv6\"]))    #4x4x64\n",
    "    \n",
    "    h_e_conv6_reshape = tf.reshape(h_e_conv6, [-1,28*23*72])\n",
    "    h_e_layer1 = layer_e1.output(h_e_conv6_reshape, tf.nn.relu)\n",
    "    h_e_layer2 = layer_e2.output(h_e_layer1, tf.nn.relu)\n",
    "#     h_e_layer = layer_e.output(h_e_conv3_reshape, tf.nn.sigmoid)\n",
    "    return h_e_layer2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(z):\n",
    "    h_d_layer1 = layer_d1.output(z, tf.nn.relu)\n",
    "    h_d_layer2 = layer_d2.output(h_d_layer1, tf.nn.relu)\n",
    "#     h_d_layer = layer_d.output(z, tf.nn.sigmoid)\n",
    "    h_d_layer_reshape = tf.reshape(h_d_layer2, [-1,28,23,72])\n",
    "    \n",
    "    output_shape_d_conv1 = tf.stack([tf.shape(z)[0], 55, 45, 72])\n",
    "    h_d_conv1 = tf.nn.relu(deconv2d(h_d_layer_reshape, decoder_var[\"W_d_conv1\"], output_shape_d_conv1, 2)+decoder_var[\"b_d_conv1\"])\n",
    "    output_shape_d_conv2 = tf.stack([tf.shape(z)[0], 55, 45, 48])\n",
    "    h_d_conv2 = tf.nn.relu(deconv2d(h_d_conv1, decoder_var[\"W_d_conv2\"], output_shape_d_conv2, 1)+decoder_var[\"b_d_conv2\"])\n",
    "\n",
    "    output_shape_d_conv3 = tf.stack([tf.shape(z)[0], 109, 89, 48])\n",
    "    h_d_conv3 = tf.nn.relu(deconv2d(h_d_conv2, decoder_var[\"W_d_conv3\"], output_shape_d_conv3, 2)+decoder_var[\"b_d_conv3\"])\n",
    "    output_shape_d_conv4 = tf.stack([tf.shape(z)[0], 109, 89, 24])\n",
    "    h_d_conv4 = tf.nn.relu(deconv2d(h_d_conv3, decoder_var[\"W_d_conv4\"], output_shape_d_conv4, 1)+decoder_var[\"b_d_conv4\"])\n",
    "\n",
    "    output_shape_d_conv5 = tf.stack([tf.shape(z)[0], 218, 178, 24])\n",
    "    h_d_conv5 = tf.nn.relu(deconv2d(h_d_conv4, decoder_var[\"W_d_conv5\"], output_shape_d_conv5, 2)+decoder_var[\"b_d_conv5\"])\n",
    "    output_shape_d_conv6 = tf.stack([tf.shape(z)[0], 218, 178, 3])\n",
    "    h_d_conv6 = tf.nn.relu(deconv2d(h_d_conv5, decoder_var[\"W_d_conv6\"], output_shape_d_conv6, 1)+decoder_var[\"b_d_conv6\"])\n",
    "    \n",
    "    return h_d_conv6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    h_g_layer1 = layer_g1.output(z, tf.nn.relu)\n",
    "    h_g_layer2 = layer_g2.output(h_g_layer1, tf.nn.relu)\n",
    "#     h_g_layer = layer_g.output(z, tf.nn.sigmoid)\n",
    "    h_g_layer_reshape = tf.reshape(h_g_layer2, [-1,28,23,72])\n",
    "    \n",
    "    output_shape_g_conv1 = tf.stack([tf.shape(z)[0], 55, 45, 72])\n",
    "    h_g_conv1 = tf.nn.relu(deconv2d(h_g_layer_reshape, generator_var[\"W_g_conv1\"], output_shape_g_conv1, 2)+generator_var[\"b_g_conv1\"])\n",
    "    output_shape_g_conv2 = tf.stack([tf.shape(z)[0], 55, 45, 48])\n",
    "    h_g_conv2 = tf.nn.relu(deconv2d(h_g_conv1, generator_var[\"W_g_conv2\"], output_shape_g_conv2, 1)+generator_var[\"b_g_conv2\"])\n",
    "\n",
    "    output_shape_g_conv3 = tf.stack([tf.shape(z)[0], 109, 89, 48])\n",
    "    h_g_conv3 = tf.nn.relu(deconv2d(h_g_conv2, generator_var[\"W_g_conv3\"], output_shape_g_conv3, 2)+generator_var[\"b_g_conv3\"])\n",
    "    output_shape_g_conv4 = tf.stack([tf.shape(z)[0], 109, 89, 24])\n",
    "    h_g_conv4 = tf.nn.relu(deconv2d(h_g_conv3, generator_var[\"W_g_conv4\"], output_shape_g_conv4, 1)+generator_var[\"b_g_conv4\"])\n",
    "\n",
    "    output_shape_g_conv5 = tf.stack([tf.shape(z)[0], 218, 178, 24])\n",
    "    h_g_conv5 = tf.nn.relu(deconv2d(h_g_conv4, generator_var[\"W_g_conv5\"], output_shape_g_conv5, 2)+generator_var[\"b_g_conv5\"])\n",
    "    output_shape_g_conv6 = tf.stack([tf.shape(z)[0], 218, 178, 3])\n",
    "    h_g_conv6 = tf.nn.relu(deconv2d(h_g_conv5, generator_var[\"W_g_conv6\"], output_shape_g_conv6, 1)+generator_var[\"b_g_conv6\"])\n",
    "    \n",
    "    return h_g_conv6\n",
    "\n",
    "def discriminator(x):\n",
    "    return decoder(encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "def loss(x):\n",
    "#     return tf.reduce_mean(tf.pow(tf.reshape(x, [-1, 784]) - tf.reshape(discriminator(x), [-1, 784]), 2))\n",
    "    return tf.reduce_mean(tf.abs(tf.reshape(x, [-1, 116412]) - tf.reshape(discriminator(x), [-1, 116412])))\n",
    "#     return tf.pow(tf.reshape(x, [-1, 784]) - tf.reshape(discriminator(x), [-1, 784]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_d = tf.placeholder(tf.float32, shape = [None, 116412])\n",
    "x_g = tf.placeholder(tf.float32, shape = [None, g_dim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.5\n",
    "# k_t = 0.\n",
    "k_t = tf.Variable(0.0, tf.float32)\n",
    "\n",
    "\n",
    "d_loss = tf.reduce_mean(loss(x_d)-k_t*loss(generator(x_g)))\n",
    "# d_loss = tf.reduce_mean(-k_t*loss(generator(x_g)))\n",
    "# d_loss = tf.reduce_mean(loss(x_d))\n",
    "\n",
    "# g_loss = loss(generator(x_g))\n",
    "g_loss = tf.reduce_mean(loss(generator(x_g)))\n",
    "\n",
    "g_sample = generator(x_g)\n",
    "\n",
    "M_global = loss(x_d) + tf.abs(gamma*loss(x_d) - loss(generator(x_g)))\n",
    "\n",
    "d_optimizer = tf.train.AdamOptimizer(0.0001).minimize(d_loss, var_list= var_d)\n",
    "g_optimizer = tf.train.AdamOptimizer(0.0001).minimize(g_loss, var_list= var_g)\n",
    "\n",
    "# balancer = gamma*loss(x_d) - loss(generator(x_g))\n",
    "balancer = tf.reduce_mean(gamma*loss(x_d) - loss(generator(x_g)))\n",
    "# update_k = k_t.assign(k_t + 0.001 * balancer)\n",
    "update_k = k_t.assign(tf.clip_by_value(k_t + 0.001 * balancer, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0  d-loss: 20.6429  g-loss: 17.1174 k_t: 0.0 M_global: 27.4389\n",
      "step: 100  d-loss: 0.414841  g-loss: 0.141414 k_t: 0.00263254 M_global: 0.481406\n",
      "step: 200  d-loss: 0.390118  g-loss: 0.1024 k_t: 0.0112017 M_global: 0.484497\n",
      "step: 300  d-loss: 0.374279  g-loss: 0.134607 k_t: 0.0190343 M_global: 0.430655\n",
      "step: 400  d-loss: 0.332595  g-loss: 0.17077 k_t: 0.0213123 M_global: 0.338887\n",
      "step: 500  d-loss: 0.306962  g-loss: 0.202924 k_t: 0.0185621 M_global: 0.358288\n",
      "step: 600  d-loss: 0.297855  g-loss: 0.19994 k_t: 0.0130766 M_global: 0.350175\n",
      "step: 700  d-loss: 0.269245  g-loss: 0.179156 k_t: 0.00829605 M_global: 0.314522\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(1001):\n",
    "    batch_x = batch(batch_size, ls)\n",
    "    z_D = sample_Z(batch_size, g_dim)\n",
    "    sess.run([d_optimizer, g_optimizer], feed_dict={x_d: batch_x, x_g: z_D})\n",
    "    z_G = sample_Z(batch_size, g_dim)\n",
    "    sess.run(g_optimizer, feed_dict={x_g: z_G})\n",
    "#     k_t = np.maximum(np.minimum(1., k_t + 0.001*(sess.run(balancer, feed_dict={x_d: batch_x, x_g: z_G}))), 0.)\n",
    "    sess.run(update_k, feed_dict={x_d: batch_x, x_g: z_G})\n",
    "    if step%100==0:\n",
    "        d_loss_train, g_loss_train, M_global_train = sess.run([d_loss, g_loss, M_global], feed_dict=\n",
    "                            {x_d: batch_x, x_g: sample_Z(batch_size, g_dim)})\n",
    "#         print 'step:', step, ' d-loss:', d_loss_train, ' g-loss:', g_loss_train, 'k_t:', k_t,\"M_global:\", M_global_train\n",
    "        print 'step:', step, ' d-loss:', d_loss_train, ' g-loss:', g_loss_train, 'k_t:', sess.run(k_t),\"M_global:\", M_global_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg_batch = batch(20, ls)\n",
    "zz = sample_Z(batch_size, g_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = sess.run(g_sample, feed_dict = {x_g: zz})\n",
    "gg_pic = np.array([np.reshape(m,(218,178,3)) for m in gg])\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3)\n",
    "for i,row in enumerate(ax):\n",
    "    for j,col in enumerate(row):\n",
    "        ax[i][j].imshow(gg_pic[i*3+j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg_pic = np.array([np.reshape(m,(218,178,3)) for m in gg_batch])\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3)\n",
    "for i,row in enumerate(ax):\n",
    "    for j,col in enumerate(row):\n",
    "        ax[i][j].imshow(gg_pic[i*3+j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
