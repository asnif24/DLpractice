{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "x_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "x_test = mnist.test.images\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for NN layers\n",
    "class layer:\n",
    "    def __init__(self, inputs, in_size, out_size, activation_function=None):\n",
    "#         self.W = tf.Variable(tf.zeros([in_size, out_size]))\n",
    "        self.W = tf.Variable(tf.random_normal([in_size, out_size]))\n",
    "#         self.b = tf.Variable(tf.zeros([1,out_size]))\n",
    "        self.b = tf.Variable(tf.constant(0.1, shape=[1,out_size]))\n",
    "        self.Wx_plus_b = tf.matmul(inputs, self.W) + self.b\n",
    "        self.activation_function = activation_function\n",
    "    def output(self):\n",
    "        if self.activation_function == None:\n",
    "            result = self.Wx_plus_b\n",
    "        else :\n",
    "            result = self.activation_function(self.Wx_plus_b)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for convolution\n",
    "class convolution:\n",
    "    def __init__(self, shape):\n",
    "        self.W = tf.Variable(tf.truncated_normal(shape=shape, stddev=0.1))\n",
    "        self.b = tf.Variable(tf.constant(0.1, shape=[shape[3]]))\n",
    "    def conv2d(self, inputs, padding='SAME'):\n",
    "        return tf.nn.conv2d(inputs, self.W, strides=[1,1,1,1], padding=padding)\n",
    "    def max_pooling_nxn(self, inputs, n):\n",
    "        return tf.nn.max_pool(inputs, ksize=[1,n,n,1], strides=[1,n,n,1], padding='SAME')\n",
    "    def conv_and_pooling(self, inputs, activation_function=None, pooling_size=2):\n",
    "        if activation_function==None:\n",
    "            h_conv =self.conv2d(inputs)+self.b\n",
    "        else:\n",
    "            h_conv = activation_function(self.conv2d(inputs)+self.b)    \n",
    "        return self.max_pooling_nxn(h_conv, pooling_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "#for dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#dealing eith inputs. \n",
    "# -1 for not considering input number of images\n",
    "# 28,28 size per iamge\n",
    "# 1 for channel. 1 for grayscale ; 3 for color image\n",
    "x_image=tf.reshape(x,[-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AlexNet => 5 convolution layers & 3 fully connected neural network\n",
    "\n",
    "conv1 = convolution([3,3,1,24])\n",
    "conv2 = convolution([3,3,24,48])\n",
    "conv3 = convolution([3,3,48,144])\n",
    "conv4 = convolution([2,2,144,288])\n",
    "conv5 = convolution([2,2,288,576])\n",
    "\n",
    "output_conv1 = conv1.conv_and_pooling(x_image, tf.nn.relu)\n",
    "output_conv2 = conv2.conv_and_pooling(output_conv1, tf.nn.relu)\n",
    "output_conv3 = conv3.conv_and_pooling(output_conv2, tf.nn.relu, pooling_size=2)\n",
    "output_conv4 = conv4.conv_and_pooling(output_conv3, tf.nn.relu, pooling_size=2)\n",
    "output_conv5 = conv5.conv_and_pooling(output_conv4, tf.nn.relu, pooling_size=2)\n",
    "\n",
    "# h_pool_flat = tf.reshape(output_conv2, [-1,7*7*40])\n",
    "h_pool_flat = tf.reshape(output_conv5, [-1,1*1*576])\n",
    "\n",
    "# layer1 = layer(h_pool_flat, 7*7*64, 500, tf.nn.relu)\n",
    "# layer1 = layer(h_pool_flat, 7*7*64, 10, tf.nn.softmax)\n",
    "# layer1 = layer(h_pool_flat, 7*7*40, 10, tf.nn.softmax)\n",
    "layer1 = layer(h_pool_flat, 1*1*576, 100, tf.nn.sigmoid)\n",
    "# layer2 = layer(layer1.output(), 400, 100, tf.nn.sigmoid)\n",
    "h_drop = tf.nn.dropout(layer1.output(), keep_prob)\n",
    "layer3 = layer(h_drop, 100, 10, tf.nn.softmax)\n",
    "# layer2 = layer(layer1.output(), 500, 100, tf.nn.relu)\n",
    "# layer3 = layer(layer2.output(), 100, 10, tf.nn.softmax)\n",
    "\n",
    "# loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=layer1.output()))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=layer3.output()))\n",
    "# train_step = tf.train.GradientDescentOptimizer(0.3).minimize(loss)\n",
    "train_step =  tf.train.MomentumOptimizer(0.003 , 0.7).minimize(loss)\n",
    "# train_step = tf.train.AdamOptimizer(0.003).minimize(loss)\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(layer3.output(), 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "batch_size = 100\n",
    "batches = x_train.shape[0]//batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start 0\n",
      "epoch: 0 loss: 1.93442 accuracy: 0.574764\n",
      "start 1\n",
      "start 2\n",
      "epoch: 2 loss: 1.63276 accuracy: 0.872655\n",
      "start 3\n",
      "start 4\n",
      "epoch: 4 loss: 1.56961 accuracy: 0.936236\n",
      "start 5\n",
      "start 6\n",
      "epoch: 6 loss: 1.53524 accuracy: 0.954618\n",
      "start 7\n",
      "start 8\n",
      "epoch: 8 loss: 1.53252 accuracy: 0.953345\n",
      "start 9\n",
      "start 10\n",
      "epoch: 10 loss: 1.51433 accuracy: 0.966273\n",
      "start 11\n",
      "start 12\n",
      "epoch: 12 loss: 1.50356 accuracy: 0.973855\n",
      "start 13\n",
      "start 14\n",
      "epoch: 14 loss: 1.50341 accuracy: 0.973527\n",
      "start 15\n",
      "start 16\n",
      "epoch: 16 loss: 1.49887 accuracy: 0.977564\n",
      "start 17\n",
      "start 18\n",
      "epoch: 18 loss: 1.494 accuracy: 0.979745\n",
      "start 19\n",
      "start 20\n",
      "epoch: 20 loss: 1.49024 accuracy: 0.982218\n",
      "start 21\n",
      "start 22\n",
      "epoch: 22 loss: 1.48781 accuracy: 0.984145\n",
      "start 23\n",
      "start 24\n",
      "epoch: 24 loss: 1.48602 accuracy: 0.985309\n",
      "start 25\n",
      "start 26\n",
      "epoch: 26 loss: 1.48408 accuracy: 0.986473\n",
      "start 27\n",
      "start 28\n",
      "epoch: 28 loss: 1.48347 accuracy: 0.986836\n",
      "start 29\n",
      "start 30\n",
      "epoch: 30 loss: 1.48211 accuracy: 0.987745\n",
      "Test accuracy: 0.9792\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(31):\n",
    "    print \"start \"+str(epoch)\n",
    "    for batch in range(batches):\n",
    "        sess.run(train_step, feed_dict={x: x_train[batch_size*batch:batch_size*(batch+1)], y: y_train[batch_size*batch:batch_size*(batch+1)], keep_prob: 0.5})\n",
    "    if epoch%2==0:\n",
    "        print \"epoch: \"+str(epoch)+\" loss: \"+str(sess.run(loss, feed_dict={x: x_train, y: y_train 0.5}))+\" accuracy: \"+str(sess.run(accuracy, feed_dict={x: x_train, y: y_train}))\n",
    "print \"Test accuracy: \"+str(sess.run(accuracy, feed_dict={x: x_test, y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start 0\n",
      "epoch: 0 loss: 1.48148 accuracy: 0.988182\n",
      "start 1\n",
      "start 2\n",
      "epoch: 2 loss: 1.4824 accuracy: 0.987291\n",
      "start 3\n",
      "start 4\n",
      "epoch: 4 loss: 1.48019 accuracy: 0.988873\n",
      "start 5\n",
      "start 6\n",
      "epoch: 6 loss: 1.47815 accuracy: 0.990273\n",
      "start 7\n",
      "start 8\n",
      "epoch: 8 loss: 1.47792 accuracy: 0.990327\n",
      "start 9\n",
      "start 10\n",
      "epoch: 10 loss: 1.47699 accuracy: 0.990927\n",
      "start 11\n",
      "start 12\n",
      "epoch: 12 loss: 1.47715 accuracy: 0.990509\n",
      "start 13\n",
      "start 14\n",
      "epoch: 14 loss: 1.47606 accuracy: 0.991582\n",
      "start 15\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(21):\n",
    "    print \"start \"+str(epoch)\n",
    "    for batch in range(batches):\n",
    "        sess.run(train_step, feed_dict={x: x_train[batch_size*batch:batch_size*(batch+1)], y: y_train[batch_size*batch:batch_size*(batch+1)]})\n",
    "    if epoch%2==0:\n",
    "        print \"epoch: \"+str(epoch)+\" loss: \"+str(sess.run(loss, feed_dict={x: x_train, y: y_train}))+\" accuracy: \"+str(sess.run(accuracy, feed_dict={x: x_train, y: y_train}))\n",
    "print \"Test accuracy: \"+str(sess.run(accuracy, feed_dict={x: x_test, y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
